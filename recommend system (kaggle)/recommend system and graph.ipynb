{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Recommender System Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the task 1 is to bulit a recommender system to recommend item to the user through what the user has purchased. In this task, I established two recommendation systems. I used the ALS algorithm for the first recommendation system, and I used Content Based Recommender for the second recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommender system base on ALS algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the data, I used a combination of train dataset and validation dataset as a training dataset, and under the recommendation system of als algorithm, I used the item_fea dataset. \n",
    "\n",
    "The item_fea data set is a data set of the characteristics of all items, and similar items can be found more accurately through the characteristics of the items.\n",
    "\n",
    "This algorithm does not use the user_fea data set. When user_fea is used, it will increase the error. Find similar users and then find items with high similarity from the purchase list of similar users and directly find items with high similarity from my purchase list. There is a big error compared to this, because similar users are not exactly the same as the users themselves, so things purchased by similar users may not be needed for users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternating Least Squares algorithms (ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import implicit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### read the data into data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature = pd.read_csv('user_fea.csv')\n",
    "train_1 = pd.read_csv('train_data.csv')\n",
    "train_2 = pd.read_csv('validation_data.csv')\n",
    "item_feature = pd.read_csv('item_fea.csv')\n",
    "test_data = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the train_data and validation_data\n",
    "train_data = pd.concat([train_1,train_2],axis = 0)\n",
    "train_data.reset_index(inplace = True, drop = True)\n",
    "# change the column name\n",
    "item_feature.rename( columns={'Unnamed: 0':'item_id'}, inplace=True )\n",
    "item_feature['item_feature'] = item_feature.iloc[:,1:].values.tolist()\n",
    "# put the item_feature into the train data\n",
    "train_data = train_data.merge(item_feature, on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column we need\n",
    "train_data = train_data[['user_id', 'item_id', 'item_feature', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two sparse matrixs\n",
    "sparse_item_user = sparse.csr_matrix((train_data['rating'].astype(float), (train_data['item_id'], train_data['user_id'])))\n",
    "sparse_user_item = sparse.csr_matrix((train_data['rating'].astype(float), (train_data['user_id'], train_data['item_id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 15\n",
    "data = (sparse_item_user * alpha).astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2a6ca063ea4f44af6235ef050446ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = implicit.als.AlternatingLeastSquares(factors=10, regularization=0.1, iterations=25)\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_id, sparse_user_item, user_vecs, item_vecs, num_items=10):\n",
    "    result_df = pd.DataFrame(columns=['user_id', 'item_id'])\n",
    "    test_item_list = test_data.loc[(test_data.user_id == user_id)]['item_id'].tolist()\n",
    "    # Get the interactions scores from the sparse user item matrix\n",
    "    user_interactions = sparse_user_item[user_id,:].toarray()\n",
    "    # Add 1 to everything, so that articles with no interaction yet become equal to 1\n",
    "    user_interactions = user_interactions.reshape(-1) + 1\n",
    "    # Make articles already interacted zero\n",
    "    user_interactions[user_interactions > 1] = 0\n",
    "    # Get dot product of user vector and all item vectors\n",
    "    rec_vector = user_vecs[user_id,:].dot(item_vecs.T).toarray()\n",
    "    # Scale this recommendation vector between 0 and 1\n",
    "    min_max = MinMaxScaler()\n",
    "    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "    # Item already interacted have their recommendation multiplied by zero\n",
    "    recommend_vector = user_interactions * rec_vector_scaled\n",
    "    # comine the item id and the recommend score\n",
    "    recommend_vector = list(zip(test_item_list,recommend_vector[test_item_list]))\n",
    "    # sort the list\n",
    "    recommend_vector_sort = sorted(recommend_vector, key = lambda x: x[1], reverse=True)\n",
    "    # get the top 10 and put them into a dataframe\n",
    "    for i in range(0,10):\n",
    "        result_df = result_df.append({'user_id':int(user_id),'item_id':int(recommend_vector_sort[i][0])},ignore_index=True)\n",
    "    result_df.user_id = result_df.user_id.astype('int64')\n",
    "    result_df.item_id = result_df.item_id.astype('int64')\n",
    "    return(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_vecs = sparse.csr_matrix(model.user_factors)\n",
    "item_vecs = sparse.csr_matrix(model.item_factors)\n",
    "result = pd.DataFrame(columns=['user_id', 'item_id'])\n",
    "for user_id in user_feature['Unnamed: 0'].tolist():\n",
    "    recommend_df = recommend(user_id, sparse_user_item, user_vecs, item_vecs, num_items=10)\n",
    "    result = pd.concat([result,recommend_df],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('ALS_result.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the result of this recommender system is 0.20756"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Content based recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The realization process of this algorithm is to calculate the average similarity between the target item and the recommended user's like and dislike items, and calculate the average similarity between the target item and the recommended user's like and dislike items. Through these four similarities Calculate the score of the recommended items, and select the top 10 items from the 100 recommended items.\n",
    "\n",
    "Cosine Similarity is used in this recommendation algorithm. Its definition is：\n",
    "$$cosine(x,y) = \\frac{x. y^\\intercal}{||x||.||y||} $$\n",
    "\n",
    "Through cosine similarity, we can get the average similarity between the recommended items and the items the user likes, the draw similarity with the item that the user does not like, the average similarity with the item that the similar user likes, and the similarity of the draw with the item that the similar user does not like. Set these four variables to user_like, user_dislike, sim_user_like, sim_user_dislke.\n",
    "\n",
    "Add user_like and sim_user_dislike and subtract the sum of user_dislike and sim_user_dislike, and use this value as the score of this item. When the score is large, it means that the item is closer to the user or similar user's favorite items. When the value is small, it means This item is closer to the item that the user or similar users do not like. Through this numerical sorting, the top 10 digits with the largest numerical value are selected as the result.\n",
    "\n",
    "I use item feature and user feature as the result of the content converted by TFIDF, and calculate the similarity matrix of all item and user through these two data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "user_feature = pd.read_csv('user_fea.csv')\n",
    "train_1 = pd.read_csv('train_data.csv')\n",
    "train_2 = pd.read_csv('validation_data.csv')\n",
    "item_feature = pd.read_csv('item_fea.csv')\n",
    "test_data = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the dataframe into a list\n",
    "user_feature_list=[]\n",
    "for i in user_feature.index.values:\n",
    "    user_feature_list.append(user_feature.iloc[i,1:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the data into a matrix\n",
    "user_feature_matrix = np.mat(user_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the comsine_similarity to calculate the similarity\n",
    "cosine_sim_user = cosine_similarity(user_feature_matrix, user_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the most similar user\n",
    "def get_sim_user(user):\n",
    "    sim_scores = list(enumerate(cosine_sim_user[user]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:6]\n",
    "    user_indices = [i[0] for i in sim_scores]\n",
    "    return user_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def a function to get the mean of the similiarity\n",
    "def get_mean_sim_item(sim_rate):\n",
    "    sim_data_list = sim_rate.tolist()\n",
    "    return np.mean(sim_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the item dataframe\n",
    "item_feature_list = []\n",
    "for i in item_feature.index.values:\n",
    "    item_feature_list.append(item_feature.iloc[i,1:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_matrix = np.mat(item_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_item = cosine_similarity(item_feature_matrix, item_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train and vaildation data set\n",
    "train_data = pd.concat([train_1,train_2],axis = 0)\n",
    "train_data.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the grade for each item\n",
    "def get_sim (user_id,target_item_id):\n",
    "    # get the user like data and user dislike data\n",
    "    user_like = train_data[(train_data.user_id == user_id)&(train_data.rating == 1)]\n",
    "    user_dislike = train_data[(train_data.user_id == user_id)&(train_data.rating == 0)]\n",
    "    # transfer them into a list\n",
    "    like_item_list = user_like.loc[:,'item_id'].tolist()\n",
    "    dislike_item_list = user_dislike.loc[:,'item_id'].tolist()\n",
    "    # use the function to get the most similar user\n",
    "    sim_user_id = get_sim_user(user_id)\n",
    "    # get the similar user like and dislike\n",
    "    sim_user_like = train_data[(train_data.user_id == sim_user_id)&(train_data.rating == 1)]\n",
    "    sim_user_dislike = train_data[(train_data.user_id == sim_user_id)&(train_data.rating == 0)]\n",
    "    # transfer them into a list\n",
    "    sim_like_item_list = sim_user_like.loc[:,'item_id'].tolist()\n",
    "    sim_dislike_item_list = user_dislike.loc[:,'item_id'].tolist()\n",
    "    # get the similiarity\n",
    "    user_simrate = cosine_sim_item[target_item_id, like_item_list]\n",
    "    # calculate the mean of the similiarity\n",
    "    user_like_simrate = get_mean_sim_item(user_simrate)\n",
    "    # get the similiarity\n",
    "    user_simrate_dis = cosine_sim_item[target_item_id, dislike_item_list]\n",
    "    # calculate the mean of the similiarity\n",
    "    user_dislike_simrate = get_mean_sim_item(user_simrate_dis)\n",
    "    # get the similiarity for similer user\n",
    "    sim_user_simrate = cosine_sim_item[target_item_id, sim_like_item_list]\n",
    "    # calculate the mean of the similiarity\n",
    "    sim_user_like_simrate = get_mean_sim_item(sim_user_simrate)\n",
    "    # get the similiarity for similer user\n",
    "    sim_user_simrate_dis = cosine_sim_item[target_item_id, sim_dislike_item_list]\n",
    "    #calculate the mean of the similiarity\n",
    "    sim_user_dislike_simrate = get_mean_sim_item(sim_user_simrate_dis)\n",
    "    # calculate the grade for the target item\n",
    "    result = user_like_simrate + sim_user_like_simrate - user_dislike_simrate - sim_user_dislike_simrate\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each item in test data, calculate the grade\n",
    "test_data['grade'] = 0\n",
    "grade_list = []\n",
    "for i in test_data.index.values:\n",
    "    user_id = test_data.loc[i,'user_id']\n",
    "    target_id = test_data.loc[i,'item_id']\n",
    "    grade_list.append(get_sim(user_id, target_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['grade'] = grade_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_list = set(test_data.loc[:,'user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe = pd.DataFrame()\n",
    "for i in user_id_list:\n",
    "    id_dataframe = test_data[(test_data.user_id == i)]\n",
    "    id_dataframe = id_dataframe.sort_values(by=\"grade\" , ascending=False)\n",
    "    recom = id_dataframe.head(10)\n",
    "    result_dataframe = pd.concat([result_dataframe, recom], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe.drop('grade', inplace = True, axis = 1)\n",
    "result_dataframe.to_csv('cos_result.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the kaggle result for this algorithms is 0.10842"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Node Classification in Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will build a graph through the networkx library, and then use node2vec vectornize all points, and then establish SVM and Logistic models to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the gtaph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the nodes data\n",
    "nodes=open('docs.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the graph\n",
    "g = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the data and create the node\n",
    "for i in nodes:\n",
    "    i = i.replace('\\n', '')\n",
    "    node_list = i.split(' ',1)\n",
    "    node_id = node_list[0]\n",
    "    node_content = node_list[1]\n",
    "    g.add_node(node_id, node_content=str(node_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the link data\n",
    "links=open('adjedges.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the data and create the link\n",
    "for l in links:\n",
    "    l = l.replace('\\n', '')\n",
    "    link_list = l.split(' ')\n",
    "    add_link = []\n",
    "    for i in range(1,len(link_list)):\n",
    "        if link_list[i]!='':\n",
    "            add_link.append((link_list[0], link_list[i]))\n",
    "    if len(add_link) != 0:\n",
    "        g.add_edges_from(add_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 36928/36928 [00:06<00:00, 5822.92it/s] \n",
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [01:13<00:00,  7.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# use the node2vec to vectornize the node\n",
    "node2vec = Node2Vec(g, dimensions=64, walk_length=10, num_walks=10, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the label into data frame and add the feature\n",
    "data_frame = pd.DataFrame(columns=['node_id', 'label', 'feature'])\n",
    "data=open('labels.txt','r')\n",
    "for k in data:\n",
    "    k = k.replace('\\n', '')\n",
    "    node_id = k.split(' ')[0]\n",
    "    label = k.split(' ')[1]\n",
    "    feature = model.wv.get_vector(node_id)\n",
    "    data_frame = data_frame.append([{'node_id':node_id, 'label':label, 'feature': feature}], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_frame, test_size = 0.8, random_state = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# transfer the data type\n",
    "feature_train_list = train['feature'].tolist()\n",
    "feature_train = np.array(feature_train_list)\n",
    "label_train_list = train['label'].tolist()\n",
    "label_train = np.array(label_train_list)\n",
    "\n",
    "feature_test_list = test['feature'].tolist()\n",
    "feature_test = np.array(feature_test_list)\n",
    "label_test_list = test['label'].tolist()\n",
    "label_test = np.array(label_test_list)\n",
    "# use the svm model\n",
    "svm_model = LinearSVC().fit(feature_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5414663461538461"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the accuracy\n",
    "svm_model.score(feature_test,label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train['label'].values.reshape(-1,1)\n",
    "test_label = test['label'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# use the logistic model\n",
    "logistic_model = LogisticRegression().fit(feature_train, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5299145299145299"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the accuracy\n",
    "logistic_model.score(feature_test,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
